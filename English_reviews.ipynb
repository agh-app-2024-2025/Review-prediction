{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hknJTfuC-rHJ"
      },
      "source": [
        "# imports and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKdQ5rW6-PU0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GRU, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.metrics import MeanAbsoluteError\n",
        "from tensorflow.keras.losses import MAE\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score,  r2_score, f1_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cje_HrpiWdUS"
      },
      "outputs": [],
      "source": [
        "#tokenization with our own solution (for research topic)\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self, num_words=None):\n",
        "        self.num_words = num_words\n",
        "        self.word_index = {}\n",
        "        self.index_word = {}\n",
        "\n",
        "    def fit_on_texts(self, texts):\n",
        "        word_counts = defaultdict(int)\n",
        "        for text in texts:\n",
        "            for word in text.split():\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "        if self.num_words:\n",
        "            sorted_words = sorted_words[:self.num_words - 1]\n",
        "\n",
        "        self.word_index = {word: i + 1 for i, (word, _) in enumerate(sorted_words)}\n",
        "        self.index_word = {i: word for word, i in self.word_index.items()}\n",
        "\n",
        "    def texts_to_sequences(self, texts):\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            sequences.append([self.word_index.get(word, 0) for word in text.split()])\n",
        "        return sequences\n",
        "\n",
        "    def pad_sequences(self, sequences, maxlen):\n",
        "        padded_sequences = np.zeros((len(sequences), maxlen), dtype=int)\n",
        "        for i, seq in enumerate(sequences):\n",
        "            if len(seq) > maxlen:\n",
        "                padded_sequences[i, :] = seq[:maxlen]\n",
        "            else:\n",
        "                padded_sequences[i, -len(seq):] = seq\n",
        "        return padded_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebEMTqQZ-NqI"
      },
      "outputs": [],
      "source": [
        "def show(df):\n",
        "  df['Rating'].plot(kind='hist', bins=20, title='Rating')\n",
        "  plt.gca().spines[['top', 'right',]].set_visible(False)\n",
        "  df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w02BgHuL-dtL"
      },
      "outputs": [],
      "source": [
        "def custom_accuracy(y_true, y_pred):\n",
        "    tolerance = 0.5\n",
        "    return np.mean(np.abs(y_true - y_pred) <= tolerance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWrefmQo-heY"
      },
      "outputs": [],
      "source": [
        "def predict_review_rating(review_text, tok_name, model_name):\n",
        "    tokenizer = joblib.load(f'{tok_name}.pkl')\n",
        "    model = load_model(f'{model_name}.h5', custom_objects={'mse': metrics.MeanSquaredError})\n",
        "\n",
        "    review_sequence = tokenizer.texts_to_sequences([review_text])\n",
        "    review_padded = pad_sequences(review_sequence, maxlen=500)\n",
        "\n",
        "    predicted_rating = model.predict(review_padded)\n",
        "    return round(predicted_rating[0][0], 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cUsvnBP-umb"
      },
      "source": [
        "# Benchmark models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F66ygSop-Fhz"
      },
      "outputs": [],
      "source": [
        "def run(data, tok_name = 'tokenizer', model_name = 'model', save = False):\n",
        "  simple_tokenizer = SimpleTokenizer(num_words=5000)\n",
        "  simple_tokenizer.fit_on_texts(data['Review'])\n",
        "  X = simple_tokenizer.texts_to_sequences(data['Review'])\n",
        "  X = simple_tokenizer.pad_sequences(X, maxlen=500)\n",
        "  y = data['Rating'].values\n",
        "\n",
        "  # tokenizer = Tokenizer(num_words=5000)\n",
        "  # tokenizer.fit_on_texts(data['Review'])\n",
        "  # X = tokenizer.texts_to_sequences(data['Review'])\n",
        "  # X = pad_sequences(X, maxlen=500)\n",
        "  # y = data['Rating'].values\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=5000, output_dim=128, input_length=500))\n",
        "  model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "  model.add(Dense(1, activation='linear'))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='mse', metrics=['accuracy','mae'])\n",
        "  history = model.fit(X_train, y_train, epochs=3, batch_size=64, validation_split=0.1)\n",
        "\n",
        "  y_pred = model.predict(X_test).flatten()\n",
        "\n",
        "  y_test_classes = np.digitize(y_test, bins = [0.5, 1.5, 2.5, 3.5, 4.5]) - 1\n",
        "  y_pred_classes = np.digitize(y_pred, bins = [0.5, 1.5, 2.5, 3.5, 4.5]) - 1\n",
        "\n",
        "  f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
        "  mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "  loss, accuracy, mae_evaluate = model.evaluate(X_test, y_test)\n",
        "  acc = custom_accuracy(y_test, y_pred)\n",
        "  print('1 - benchmark')\n",
        "  print(f'MAE on Test Set: {mae}')\n",
        "  print(f'MAE_ev on Test Set: {mae_evaluate}')\n",
        "  print(f'Accuracy on Test Set: {accuracy}')\n",
        "  print(f'F1 Score on Test Set: {f1:.2f}')\n",
        "  print(f'Custom Accuracy (within tolerance): {acc * 100:.2f}%')\n",
        "\n",
        "  if save:\n",
        "    joblib.dump(simple_tokenizer, f'{tok_name}.pkl')\n",
        "    model.save(f'{model_name}.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPWSH92_-IkG"
      },
      "outputs": [],
      "source": [
        "def run2(data, tok_name='tokenizer', model_name='model', save=False):\n",
        "    simple_tokenizer = SimpleTokenizer(num_words=5000)\n",
        "    simple_tokenizer.fit_on_texts(data['Review'])\n",
        "    X = simple_tokenizer.texts_to_sequences(data['Review'])\n",
        "    X = simple_tokenizer.pad_sequences(X, maxlen=500)\n",
        "    y = data['Rating'].values\n",
        "\n",
        "\n",
        "    # tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "    # tokenizer.fit_on_texts(data['Review'])\n",
        "    # X = tokenizer.texts_to_sequences(data['Review'])\n",
        "    # X = pad_sequences(X, maxlen=500)\n",
        "    # y = data['Rating'].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=5000, output_dim=128, input_length=500),\n",
        "        LSTM(64, dropout=0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='relu')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mae', metrics=['accuracy','mae'])\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=20, batch_size=64,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stop]\n",
        "    )\n",
        "\n",
        "    y_pred = model.predict(X_test).flatten()\n",
        "\n",
        "    y_test_classes = np.digitize(y_test, bins = [0.5, 1.5, 2.5, 3.5, 4.5]) - 1\n",
        "    y_pred_classes = np.digitize(y_pred, bins = [0.5, 1.5, 2.5, 3.5, 4.5]) - 1\n",
        "\n",
        "    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    loss, accuracy, mae_evaluate = model.evaluate(X_test, y_test)\n",
        "    acc = custom_accuracy(y_test, y_pred)\n",
        "    print('2 - benchmark')\n",
        "    print(f'MAE on Test Set: {mae}')\n",
        "    print(f'MAE_ev on Test Set: {mae_evaluate}')\n",
        "    print(f'Accuracy on Test Set: {accuracy}')\n",
        "    print(f'F1 Score on Test Set: {f1:.2f}')\n",
        "    print(f'Custom Accuracy (within tolerance): {acc * 100:.2f}%')\n",
        "\n",
        "    if save:\n",
        "      joblib.dump(simple_tokenizer, f'{tok_name}.pkl')\n",
        "      model.save(f'{model_name}.h5')\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO4_-T0jaPTM"
      },
      "source": [
        "#Benchmark models with integer outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zthFYH6BDKVM"
      },
      "outputs": [],
      "source": [
        "def run_int(data, tok_name='tokenizer', model_name='model', save=False):\n",
        "    simple_tokenizer = SimpleTokenizer(num_words=5000)\n",
        "    simple_tokenizer.fit_on_texts(data['Review'])\n",
        "    X = simple_tokenizer.texts_to_sequences(data['Review'])\n",
        "    X = simple_tokenizer.pad_sequences(X, maxlen=500)\n",
        "    y = data['Rating'].values - 1\n",
        "\n",
        "    # tokenizer = Tokenizer(num_words=5000)\n",
        "    # tokenizer.fit_on_texts(data['Review'])\n",
        "    # X = tokenizer.texts_to_sequences(data['Review'])\n",
        "    # X = pad_sequences(X, maxlen=500)\n",
        "    # y = data['Rating'].values - 1  # Convert 1-10 ratings to 0-9\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=5000, output_dim=128, input_length=500))\n",
        "    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(X_train, y_train, epochs=3, batch_size=64, validation_split=0.1)\n",
        "\n",
        "    y_pred_classes = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "    print('1 na int')\n",
        "    print(f'F1 Score on Test Set: {f1:.2f}')\n",
        "    print(f'Accuracy on Test Set: {accuracy:.2f}')\n",
        "\n",
        "    if save:\n",
        "        joblib.dump(simple_tokenizer, f'{tok_name}.pkl')\n",
        "        model.save(f'{model_name}.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwrSzuIzFfXc"
      },
      "outputs": [],
      "source": [
        "def run2_int(data, tok_name='tokenizer', model_name='model', save=False):\n",
        "    simple_tokenizer = SimpleTokenizer(num_words=5000)\n",
        "    simple_tokenizer.fit_on_texts(data['Review'])\n",
        "    X = simple_tokenizer.texts_to_sequences(data['Review'])\n",
        "    X = simple_tokenizer.pad_sequences(X, maxlen=500)\n",
        "    y = data['Rating'].values - 1\n",
        "\n",
        "    # tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "    # tokenizer.fit_on_texts(data['Review'])\n",
        "    # X = tokenizer.texts_to_sequences(data['Review'])\n",
        "    # X = pad_sequences(X, maxlen=500)\n",
        "    # y = data['Rating'].values - 1\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=5000, output_dim=128, input_length=500),\n",
        "        LSTM(64, dropout=0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=20, batch_size=64,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stop]\n",
        "    )\n",
        "\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "    print('2 na int')\n",
        "    print(f'Accuracy on Test Set: {accuracy}')\n",
        "    print(f'F1 Score on Test Set: {f1:.2f}')\n",
        "\n",
        "    if save:\n",
        "        joblib.dump(simple_tokenizer, f'{tok_name}.pkl')\n",
        "        model.save(f'{model_name}.h5')\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p288JXk4Ls4z"
      },
      "outputs": [],
      "source": [
        "def run2_bi(data, tok_name='tokenizer', model_name='model', save=False):\n",
        "    simple_tokenizer = SimpleTokenizer(num_words=5000)\n",
        "    simple_tokenizer.fit_on_texts(data['Review'])\n",
        "    X = simple_tokenizer.texts_to_sequences(data['Review'])\n",
        "    X = simple_tokenizer.pad_sequences(X, maxlen=500)\n",
        "    y = data['Rating'].values - 1\n",
        "\n",
        "\n",
        "    # tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "    # tokenizer.fit_on_texts(data['Review'])\n",
        "    # X = tokenizer.texts_to_sequences(data['Review'])\n",
        "    # X = pad_sequences(X, maxlen=500)\n",
        "    # y = data['Rating'].values - 1\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = Sequential([\n",
        "      Embedding(input_dim=10000, output_dim=128, input_length=500),\n",
        "      Bidirectional(LSTM(128, dropout=0.2, return_sequences=True)),\n",
        "      LSTM(64, dropout=0.2),\n",
        "      Dense(64, activation='relu'),\n",
        "      Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=20, batch_size=64,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stop]\n",
        "    )\n",
        "\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "    print('Bi')\n",
        "    print(f'Accuracy on Test Set: {accuracy}')\n",
        "    print(f'F1 Score on Test Set: {f1:.2f}')\n",
        "\n",
        "    if save:\n",
        "        joblib.dump(simple_tokenizer, f'{tok_name}.pkl')\n",
        "        model.save(f'{model_name}.h5')\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mic4dOKjaL5T"
      },
      "source": [
        "# GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmA3-tCIadWo"
      },
      "outputs": [],
      "source": [
        "def run2_gru(data, tok_name='tokenizer', model_name='model', save=False):\n",
        "    simple_tokenizer = SimpleTokenizer(num_words=5000)\n",
        "    simple_tokenizer.fit_on_texts(data['Review'])\n",
        "    X = simple_tokenizer.texts_to_sequences(data['Review'])\n",
        "    X = simple_tokenizer.pad_sequences(X, maxlen=500)\n",
        "    y = data['Rating'].values - 1\n",
        "\n",
        "\n",
        "    # tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "    # tokenizer.fit_on_texts(data['Review'])\n",
        "    # X = tokenizer.texts_to_sequences(data['Review'])\n",
        "    # X = pad_sequences(X, maxlen=500)\n",
        "    # y = data['Rating'].values - 1\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=5000, output_dim=128, input_length=500),\n",
        "        GRU(128, dropout=0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=20, batch_size=64,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stop]\n",
        "    )\n",
        "\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "    print('GRU')\n",
        "    print(f'Accuracy on Test Set: {accuracy}')\n",
        "    print(f'F1 Score on Test Set: {f1:.2f}')\n",
        "\n",
        "    if save:\n",
        "        joblib.dump(simple_tokenizer, f'{tok_name}.pkl')\n",
        "        model.save(f'{model_name}.h5')\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMOyMIqKXm8E"
      },
      "source": [
        "# CNN + RNN hybrid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUy95bNOXqjY"
      },
      "outputs": [],
      "source": [
        "def run2_cnn_rnn(data, tok_name='tokenizer', model_name='model', save=False):\n",
        "    simple_tokenizer = SimpleTokenizer(num_words=5000)\n",
        "    simple_tokenizer.fit_on_texts(data['Review'])\n",
        "    X = simple_tokenizer.texts_to_sequences(data['Review'])\n",
        "    X = simple_tokenizer.pad_sequences(X, maxlen=500)\n",
        "    y = data['Rating'].values - 1\n",
        "\n",
        "\n",
        "    # Tokenizer setup\n",
        "    # tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "    # tokenizer.fit_on_texts(data['Review'])\n",
        "    # X = tokenizer.texts_to_sequences(data['Review'])\n",
        "    # X = pad_sequences(X, maxlen=500)\n",
        "    # y = data['Rating'].values - 1\n",
        "\n",
        "    # Split into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # CNN + RNN Hybrid Model Architecture\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=5000, output_dim=128, input_length=500),\n",
        "        Conv1D(128, kernel_size=5, activation='relu'),\n",
        "        MaxPooling1D(pool_size=4),\n",
        "        LSTM(64, dropout=0.2, return_sequences=False),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=20, batch_size=64,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stop]\n",
        "    )\n",
        "\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "    print('cnn+rnn')\n",
        "    print(f'Accuracy on Test Set: {accuracy}')\n",
        "    print(f'F1 Score on Test Set: {f1:.2f}')\n",
        "\n",
        "    if save:\n",
        "        joblib.dump(simple_tokenizer, f'{tok_name}.pkl')\n",
        "        model.save(f'{model_name}.h5')\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZo1Rc4dXeL4"
      },
      "source": [
        "#SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuCNsJVtXfxU"
      },
      "outputs": [],
      "source": [
        "def run2_svm(data, vectorizer_name='tfidf_vectorizer', model_name='svm_model', save=False):\n",
        "\n",
        "    reviews = data['Review'].tolist()\n",
        "    ratings = data['Rating'].values - 1\n",
        "\n",
        "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "    X = vectorizer.fit_transform(reviews).toarray()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = SVC(kernel='linear', C=1, probability=True)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    print('SVC')\n",
        "    print(f'Accuracy on Test Set: {accuracy}')\n",
        "    print(f'F1 Score on Test Set: {f1:.2f}')\n",
        "\n",
        "    if save:\n",
        "        joblib.dump(vectorizer, f'{vectorizer_name}.pkl')\n",
        "        joblib.dump(model, f'{model_name}.pkl')\n",
        "\n",
        "    return model, vectorizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1fwYP830TWA"
      },
      "source": [
        "# LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfE1AuI40WdD"
      },
      "outputs": [],
      "source": [
        "def run_logistic_regression(data, target_col='Rating', text_col='Review', save=False, model_name='logistic_model'):\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(data[target_col])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data[text_col], y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('vectorizer', CountVectorizer(max_features=5000)),\n",
        "        ('classifier', LogisticRegression(max_iter=1000, solver='lbfgs', random_state=42))\n",
        "    ])\n",
        "\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    print(\"run_logistic_regression\")\n",
        "    print(f'Accuracy: {acc}')\n",
        "    print(f'F1 Score: {f1:.2f}')\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "    if save:\n",
        "        joblib.dump(pipeline, f'{model_name}.pkl')\n",
        "        joblib.dump(label_encoder, f'{model_name}_label_encoder.pkl')\n",
        "\n",
        "    return pipeline, label_encoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUA88mEG2jlX"
      },
      "source": [
        "# run all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmFFsQ5Z3wHt"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/ang_full.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfplU73I32hN",
        "outputId": "f90e1ec4-249e-4497-9700-c155634da4b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-44-9a4fffe4ca1c>:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.sample(n=3000, random_state=42) if len(x) >= 3000 else x)\n"
          ]
        }
      ],
      "source": [
        "sampled_data = (\n",
        "    df.groupby('Rating', group_keys=False)\n",
        "    .apply(lambda x: x.sample(n=3000, random_state=42) if len(x) >= 3000 else x)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "d1roz5lIAAII",
        "outputId": "f22979cd-6b2a-4e7a-b085-37f7c3cc740c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27966 entries, 0 to 27965\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Unnamed: 0  27966 non-null  int64  \n",
            " 1   Book Title  27966 non-null  object \n",
            " 2   Review      27966 non-null  object \n",
            " 3   Rating      27966 non-null  float64\n",
            "dtypes: float64(1), int64(1), object(2)\n",
            "memory usage: 874.1+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "show(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWObG-JJ-owv",
        "outputId": "aeb9a20b-4c4d-4ffc-cb8f-79413a5d8ff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 688ms/step - accuracy: 0.1660 - loss: 3.8148 - mae: 1.5893 - val_accuracy: 0.1850 - val_loss: 1.8466 - val_mae: 1.1564\n",
            "Epoch 2/3\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 704ms/step - accuracy: 0.2022 - loss: 1.8580 - mae: 1.1693 - val_accuracy: 0.1842 - val_loss: 1.6971 - val_mae: 1.0796\n",
            "Epoch 3/3\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 687ms/step - accuracy: 0.1972 - loss: 1.4313 - mae: 0.9862 - val_accuracy: 0.1850 - val_loss: 1.4453 - val_mae: 0.9739\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step - accuracy: 0.2083 - loss: 1.5907 - mae: 1.0338\n",
            "1 - benchmark\n",
            "MAE on Test Set: 1.026981767853101\n",
            "MAE_ev on Test Set: 1.0269818305969238\n",
            "Accuracy on Test Set: 0.19966666400432587\n",
            "F1 Score on Test Set: 0.24\n",
            "Custom Accuracy (within tolerance): 28.77%\n"
          ]
        }
      ],
      "source": [
        "run(sampled_data, \"tokenizer_clean\", \"model_clean\", False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CISdR-GJr5Q9",
        "outputId": "d71236f7-05f4-458a-f828-cbbfc81e7345"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 671ms/step - accuracy: 0.1637 - loss: 1.5911 - mae: 1.5911 - val_accuracy: 0.1850 - val_loss: 1.1570 - val_mae: 1.1570\n",
            "Epoch 2/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 682ms/step - accuracy: 0.1998 - loss: 1.1670 - mae: 1.1670 - val_accuracy: 0.1850 - val_loss: 1.0465 - val_mae: 1.0465\n",
            "Epoch 3/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 654ms/step - accuracy: 0.1985 - loss: 0.9573 - mae: 0.9573 - val_accuracy: 0.1858 - val_loss: 0.9553 - val_mae: 0.9553\n",
            "Epoch 4/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 692ms/step - accuracy: 0.2022 - loss: 0.8505 - mae: 0.8505 - val_accuracy: 0.1858 - val_loss: 0.9306 - val_mae: 0.9306\n",
            "Epoch 5/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 673ms/step - accuracy: 0.2025 - loss: 0.7449 - mae: 0.7449 - val_accuracy: 0.1858 - val_loss: 0.9417 - val_mae: 0.9417\n",
            "Epoch 6/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 678ms/step - accuracy: 0.2019 - loss: 0.6867 - mae: 0.6867 - val_accuracy: 0.1858 - val_loss: 0.9131 - val_mae: 0.9131\n",
            "Epoch 7/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 657ms/step - accuracy: 0.2066 - loss: 0.6325 - mae: 0.6325 - val_accuracy: 0.1858 - val_loss: 0.9664 - val_mae: 0.9664\n",
            "Epoch 8/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 672ms/step - accuracy: 0.1994 - loss: 0.5634 - mae: 0.5634 - val_accuracy: 0.1858 - val_loss: 0.9183 - val_mae: 0.9183\n",
            "Epoch 9/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 687ms/step - accuracy: 0.2036 - loss: 0.5200 - mae: 0.5200 - val_accuracy: 0.1858 - val_loss: 0.8881 - val_mae: 0.8881\n",
            "Epoch 10/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 672ms/step - accuracy: 0.1967 - loss: 0.4838 - mae: 0.4838 - val_accuracy: 0.1858 - val_loss: 0.9360 - val_mae: 0.9360\n",
            "Epoch 11/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 677ms/step - accuracy: 0.2063 - loss: 0.4475 - mae: 0.4475 - val_accuracy: 0.1858 - val_loss: 0.9411 - val_mae: 0.9411\n",
            "Epoch 12/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 700ms/step - accuracy: 0.1977 - loss: 0.4127 - mae: 0.4127 - val_accuracy: 0.1858 - val_loss: 0.9252 - val_mae: 0.9252\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 79ms/step\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 88ms/step - accuracy: 0.2083 - loss: 0.9634 - mae: 0.9634\n",
            "2 - benchmark\n",
            "MAE on Test Set: 0.9387723729809125\n",
            "MAE_ev on Test Set: 0.9387719035148621\n",
            "Accuracy on Test Set: 0.19966666400432587\n",
            "F1 Score on Test Set: 0.35\n",
            "Custom Accuracy (within tolerance): 35.20%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(<Sequential name=sequential_22, built=True>,\n",
              " <keras.src.callbacks.history.History at 0x78adbae5f1c0>)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run2(sampled_data, \"tokenizer_clean\", \"model_clean\", False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaGaXk0ZDN-Z",
        "outputId": "0c868168-4807-4a8a-c515-de7ec5a16d21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 674ms/step - accuracy: 0.2282 - loss: 1.5952 - val_accuracy: 0.2167 - val_loss: 1.6405\n",
            "Epoch 2/3\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 669ms/step - accuracy: 0.3305 - loss: 1.5317 - val_accuracy: 0.3625 - val_loss: 1.4177\n",
            "Epoch 3/3\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 682ms/step - accuracy: 0.3895 - loss: 1.4089 - val_accuracy: 0.3758 - val_loss: 1.3942\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 93ms/step\n",
            "1 na int\n",
            "F1 Score on Test Set: 0.34\n",
            "Accuracy on Test Set: 0.36\n"
          ]
        }
      ],
      "source": [
        "run_int(sampled_data, \"tokenizer_clean\", \"model_clean\", False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsUu7YntFjbi",
        "outputId": "31700342-7af8-4ef7-bbbf-ed7b1242f842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 668ms/step - accuracy: 0.2310 - loss: 1.5991 - val_accuracy: 0.3408 - val_loss: 1.4325\n",
            "Epoch 2/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 657ms/step - accuracy: 0.3888 - loss: 1.3834 - val_accuracy: 0.3708 - val_loss: 1.3791\n",
            "Epoch 3/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 645ms/step - accuracy: 0.4554 - loss: 1.2493 - val_accuracy: 0.3850 - val_loss: 1.3799\n",
            "Epoch 4/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 639ms/step - accuracy: 0.5464 - loss: 1.0827 - val_accuracy: 0.4167 - val_loss: 1.3758\n",
            "Epoch 5/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 642ms/step - accuracy: 0.6252 - loss: 0.9430 - val_accuracy: 0.4150 - val_loss: 1.4071\n",
            "Epoch 6/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 667ms/step - accuracy: 0.6760 - loss: 0.8311 - val_accuracy: 0.4158 - val_loss: 1.5292\n",
            "Epoch 7/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 647ms/step - accuracy: 0.7313 - loss: 0.7102 - val_accuracy: 0.4192 - val_loss: 1.6814\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step\n",
            "2 na int\n",
            "Accuracy on Test Set: 0.4103333333333333\n",
            "F1 Score on Test Set: 0.39\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(<Sequential name=sequential_24, built=True>,\n",
              " <keras.src.callbacks.history.History at 0x78adae2ce950>)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run2_int(sampled_data, \"tokenizer_clean\", \"model_clean\", False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiVtK0IBMPqf",
        "outputId": "c882f1d4-cffa-4cc4-d596-948758902df0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m545s\u001b[0m 3s/step - accuracy: 0.2288 - loss: 1.5963 - val_accuracy: 0.3092 - val_loss: 1.4742\n",
            "Epoch 2/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 3s/step - accuracy: 0.3215 - loss: 1.4721 - val_accuracy: 0.3183 - val_loss: 1.5340\n",
            "Epoch 3/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 3s/step - accuracy: 0.3621 - loss: 1.4084 - val_accuracy: 0.3975 - val_loss: 1.3499\n",
            "Epoch 4/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 3s/step - accuracy: 0.4637 - loss: 1.2076 - val_accuracy: 0.4017 - val_loss: 1.3907\n",
            "Epoch 5/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 3s/step - accuracy: 0.5427 - loss: 1.0714 - val_accuracy: 0.4008 - val_loss: 1.4170\n",
            "Epoch 6/20\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m566s\u001b[0m 3s/step - accuracy: 0.5827 - loss: 1.0082 - val_accuracy: 0.3992 - val_loss: 1.4908\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 644ms/step\n",
            "Bi\n",
            "Accuracy on Test Set: 0.39\n",
            "F1 Score on Test Set: 0.37\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(<Sequential name=sequential_25, built=True>,\n",
              " <keras.src.callbacks.history.History at 0x78adae135c30>)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run2_bi(sampled_data, \"tokenizer_clean\", \"model_clean\", False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TViO61P_arXH"
      },
      "outputs": [],
      "source": [
        "run2_gru(sampled_data, \"tokenizer_clean\", \"model_clean\", False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzUvv7tSYw7u"
      },
      "outputs": [],
      "source": [
        "run2_cnn_rnn(sampled_data, \"tokenizer_clean\", \"model_clean\", False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUrm3lz3dVYj",
        "outputId": "eec92efe-6c79-4fff-f31c-c79745042c7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVC\n",
            "Accuracy on Test Set: 0.547372184483375\n",
            "F1 Score on Test Set: 0.52\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(SVC(C=1, kernel='linear', probability=True),\n",
              " TfidfVectorizer(max_features=5000, stop_words='english'))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run2_svm(df, \"tokenizer_clean\", \"model_clean\", False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP6dPgsi2cfD",
        "outputId": "db63a82f-74c5-480d-e85c-7a9833ae4967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run_logistic_regression\n",
            "Accuracy: 0.5067929924919556\n",
            "F1 Score: 0.50\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.47      0.50       520\n",
            "           1       0.32      0.27      0.29       461\n",
            "           2       0.29      0.30      0.29       661\n",
            "           3       0.39      0.39      0.39      1453\n",
            "           4       0.65      0.68      0.66      2499\n",
            "\n",
            "    accuracy                           0.51      5594\n",
            "   macro avg       0.44      0.42      0.43      5594\n",
            "weighted avg       0.50      0.51      0.50      5594\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pipeline, label_encoder = run_logistic_regression(df, save = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NZo1Rc4dXeL4",
        "Y1fwYP830TWA"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}